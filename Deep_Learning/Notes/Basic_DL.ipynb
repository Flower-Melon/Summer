{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12f2d359",
   "metadata": {},
   "source": [
    "# 此文档用于动手实现pytorch的一些基础功能\n",
    "* 包括线性回归，sofatmax回归，MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b08215f",
   "metadata": {},
   "source": [
    "## 1.线性回归的pytorch实现\n",
    "* 不使用api的原始实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "357b760d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 \n",
      " loss: 2.1297317\n",
      "epoch: 2 \n",
      " loss: 0.26636225\n",
      "epoch: 3 \n",
      " loss: 0.03388089\n",
      "epoch: 4 \n",
      " loss: 0.0044209873\n",
      "epoch: 5 \n",
      " loss: 0.0006264636\n",
      "w: tensor([[ 1.9841],\n",
      "        [-3.3899]], requires_grad=True) \n",
      " b: tensor([4.1712], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch as T\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 数据生成器\n",
    "def generate_data(w,b,num):\n",
    "    X = T.normal(0,1,size=(num,len(w)))\n",
    "    y = T.matmul(X,w) + b\n",
    "    y += T.normal(0,0.01,size=(num,1))\n",
    "    return X,y\n",
    "\n",
    "# 数据输出器\n",
    "def data_iter(batch_size,features,labels):\n",
    "    num = len(features)\n",
    "    index = [i for i in range(num)]\n",
    "    random.shuffle(index) # 随机打乱数据\n",
    "    for i in range(0,num,batch_size):\n",
    "        batch_index = T.tensor(index[i:min(i + batch_size,num)])\n",
    "        yield features[batch_index],labels[batch_index] #按batch_size输出\n",
    "\n",
    "# 定义模型\n",
    "def line(X,w,b):\n",
    "    return T.matmul(X,w) + b\n",
    "\n",
    "# 定义损失函数\n",
    "def loss(y_hat,y):\n",
    "    return ((y_hat-y)**2)/2\n",
    "\n",
    "# 定义优化算法\n",
    "def sgd (paras,lr,batch_size):\n",
    "    with T.no_grad():\n",
    "        for para in paras:\n",
    "            para -= lr * para.grad/batch_size\n",
    "            para.grad.zero_() # 用于清零当前参数的梯度，以便在下一次反向传播时重新计算\n",
    "\n",
    "# 真实数据\n",
    "true_w = T.tensor([[2],[-3.4]])\n",
    "true_b = T.tensor(4.2)\n",
    "\n",
    "features,labels = generate_data(true_w,true_b,1000)\n",
    "# plt只能画数组型的数据，需要使用.numpy()转化\n",
    "# plt.scatter(features[:,1].numpy(),labels.numpy(),1)\n",
    "\n",
    "batch_size = 10 # batch大小\n",
    "w = T.zeros(size=(2,1),requires_grad=True)\n",
    "b = T.zeros(1,requires_grad=True)\n",
    "\n",
    "# 训练\n",
    "lr = 0.01\n",
    "epoch = 5\n",
    "\n",
    "for i in range(epoch):\n",
    "    for X,y in data_iter(batch_size,features,labels):\n",
    "        l = loss(line(X,w,b),y)\n",
    "        l.sum().backward()\n",
    "        sgd([w,b],lr,batch_size)\n",
    "    with T.no_grad():\n",
    "        tran_l = loss(line(features,w,b),labels)\n",
    "        print(\"epoch:\",i + 1,\"\\n\",\"loss:\",tran_l.mean().numpy())\n",
    "\n",
    "print(\"w:\",w,\"\\n\",\"b:\",b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037536a3",
   "metadata": {},
   "source": [
    "* 简洁实现版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c51ee1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.0002, -3.3924]])\n",
      "epoch: 1 \n",
      " loss: 0.00023145585\n",
      "tensor([[ 2.0010, -3.4002]])\n",
      "epoch: 2 \n",
      " loss: 0.00010563044\n",
      "tensor([[ 2.0006, -3.4007]])\n",
      "epoch: 3 \n",
      " loss: 0.000105609295\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch as T\n",
    "from torch.utils import data\n",
    "from torch import nn\n",
    "\n",
    "# 数据生成器\n",
    "def generate_data(w,b,num):\n",
    "    X = T.normal(0,1,size=(num,len(w)))\n",
    "    y = T.matmul(X,w) + b\n",
    "    y += T.normal(0,0.01,size=(num,1))\n",
    "    return X,y\n",
    "\n",
    "# 读取数据集(这里使用高级api实现)\n",
    "def load_array(features,labels,batch_size):\n",
    "    data_set = data.TensorDataset(features,labels)\n",
    "    return data.DataLoader(data_set,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "# 真实数据\n",
    "true_w = T.tensor([[2],[-3.4]])\n",
    "true_b = T.tensor(4.2)\n",
    "\n",
    "# 数据生成器定义\n",
    "features,labels = generate_data(true_w,true_b,1000)\n",
    "data_iter = load_array(features,labels,10)\n",
    "\n",
    "# 网络定义\n",
    "net = nn.Sequential(nn.Linear(2,1))\n",
    "# 参数初始化\n",
    "net[0].weight.data.normal_(0,0.01)\n",
    "net[0].bias.data.fill_(0)\n",
    "\n",
    "# 定义损失函数\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "# 定义优化算法\n",
    "trainer = T.optim.SGD(net.parameters(),lr=0.03)\n",
    "\n",
    "# 训练\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    for X,y in data_iter:\n",
    "        l = loss(net(X),y)\n",
    "        # 以下三步可以说是封装的非常简洁了\n",
    "        trainer.zero_grad()\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "    # with这句话非常关键，不然会Can't call numpy() on Tensor that requires grad.\n",
    "    # with T.no_grad():\n",
    "    tran_l = loss(net(features),labels)\n",
    "    print(net[0].weight.data)\n",
    "    print(\"epoch:\",epoch + 1,\"\\n\",\"loss:\",tran_l.detach().numpy())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802056ad",
   "metadata": {},
   "source": [
    "## 2.softmax回归的pytorch实现\n",
    "* 原始实现\n",
    "（交叉熵是度量两个概率分布差异的很好的度量，而平方差损失度量的是两个样本之间的差异）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c3c34e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 \n",
      " loss: 2.3242693 \n",
      " test accuracy: 0.6406\n",
      "epoch: 2 \n",
      " loss: 1.3307658 \n",
      " test accuracy: 0.7429\n",
      "epoch: 3 \n",
      " loss: 1.0492113 \n",
      " test accuracy: 0.7832\n",
      "epoch: 4 \n",
      " loss: 0.97587156 \n",
      " test accuracy: 0.8077\n",
      "epoch: 5 \n",
      " loss: 1.168331 \n",
      " test accuracy: 0.821\n",
      "epoch: 6 \n",
      " loss: 0.8169982 \n",
      " test accuracy: 0.8329\n",
      "epoch: 7 \n",
      " loss: 0.94888455 \n",
      " test accuracy: 0.842\n",
      "epoch: 8 \n",
      " loss: 0.5684145 \n",
      " test accuracy: 0.8463\n",
      "epoch: 9 \n",
      " loss: 0.57723504 \n",
      " test accuracy: 0.8509\n",
      "epoch: 10 \n",
      " loss: 0.5201943 \n",
      " test accuracy: 0.8552\n"
     ]
    }
   ],
   "source": [
    "import torch as T\n",
    "import torchvision as tv\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "\n",
    "# 定义数据生成器\n",
    "def load_data(batch_size):\n",
    "    # 将图像从 PIL 格式或 NumPy 数组转换为 PyTorch 的张量（tensor）\n",
    "    trans = [transforms.ToTensor()]\n",
    "    # 是一个用于将多个转换操作组合在一起的工具\n",
    "    trans = transforms.Compose(trans)\n",
    "    # 加载mnist数据集\n",
    "    train_data = tv.datasets.MNIST(root='./data',train=True,download=True,transform=trans)\n",
    "    test_data = tv.datasets.MNIST(root='./data',train=False,download=True,transform=trans)\n",
    "    # 定义数据加载器\n",
    "    train_iter = data.DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
    "    test_iter = data.DataLoader(test_data,batch_size=batch_size,shuffle=False)\n",
    "    return train_iter,test_iter\n",
    "\n",
    "# 定义softmax函数\n",
    "def softmax(X):\n",
    "    X_exp = T.exp(X)\n",
    "    partition = X_exp.sum(dim=1,keepdim=True)\n",
    "    return X_exp / partition # 广播机制，很好用\n",
    "\n",
    " # 定义模型\n",
    "def net(X):\n",
    "    return softmax(T.matmul(X.reshape(-1,num_inputs),w) + b)\n",
    "\n",
    "# 定义损失函数\n",
    "def cross_entropy(y_hat,y):\n",
    "    return -T.log(y_hat[T.arange(len(y_hat)),y])\n",
    "\n",
    "# 定义优化算法\n",
    "def sgd(params,lr,batch_size):\n",
    "    with T.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad/batch_size\n",
    "            param.grad.zero_()\n",
    "\n",
    "# 超参数\n",
    "epoch = 10\n",
    "lr = 0.1\n",
    "batch_size = 256\n",
    "train_iter,test_iter = load_data(batch_size)\n",
    "# 确定输入和输出\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "# 参数初始化\n",
    "w = T.randn(num_inputs,num_outputs,requires_grad=True)\n",
    "b = T.zeros(num_outputs,requires_grad=True)\n",
    "# 进行训练\n",
    "for i in range(epoch):\n",
    "    for X,y in train_iter:\n",
    "        y_hat = net(X)\n",
    "        loss = cross_entropy(y_hat,y)\n",
    "        loss.sum().backward()\n",
    "        sgd([w,b],lr,batch_size)\n",
    "    with T.no_grad():\n",
    "        # 计算损失\n",
    "        tran_l = cross_entropy(net(X),y)\n",
    "        # 在测试集上计算分类正确率\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for X, y in test_iter:\n",
    "            y_hat = net(X)\n",
    "            predicted = T.argmax(y_hat, dim=1)  # 获取预测标签\n",
    "            correct += (predicted == y).sum().item()  # 统计正确的预测数量\n",
    "            total += y.size(0)  # 统计总样本数量\n",
    "        accuracy = correct / total\n",
    "        print(\"epoch:\",i + 1,\"\\n\",\"loss:\",tran_l.mean().numpy(),\"\\n\",\n",
    "              \"test accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c952bf90",
   "metadata": {},
   "source": [
    "* 索引关系，tensor和ndarray的索引关系是一致的,都可以使用tensor,ndarrdy,list和range型进行索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a142aa19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2])\n",
      "[1 2]\n"
     ]
    }
   ],
   "source": [
    "import torch as T\n",
    "\n",
    "a = np.arange(10)\n",
    "b = T.arange(10)\n",
    "c = [i for i in range(10)]\n",
    "d = T.tensor([1,2])\n",
    "e = np.array([1,2,3])\n",
    "f = range(3)\n",
    "\n",
    "print(b[f])\n",
    "print(a[d])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da00086",
   "metadata": {},
   "source": [
    "* 简洁实现版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7f8711e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 \n",
      " loss: 0.40087435 \n",
      " test accuracy: 0.8838\n",
      "epoch: 2 \n",
      " loss: 0.2801941 \n",
      " test accuracy: 0.8963\n",
      "epoch: 3 \n",
      " loss: 0.36306193 \n",
      " test accuracy: 0.9052\n",
      "epoch: 4 \n",
      " loss: 0.39751872 \n",
      " test accuracy: 0.9072\n",
      "epoch: 5 \n",
      " loss: 0.31081054 \n",
      " test accuracy: 0.9109\n",
      "epoch: 6 \n",
      " loss: 0.19952668 \n",
      " test accuracy: 0.9128\n",
      "epoch: 7 \n",
      " loss: 0.3792193 \n",
      " test accuracy: 0.9159\n",
      "epoch: 8 \n",
      " loss: 0.20895837 \n",
      " test accuracy: 0.9154\n",
      "epoch: 9 \n",
      " loss: 0.27070248 \n",
      " test accuracy: 0.9163\n",
      "epoch: 10 \n",
      " loss: 0.3738365 \n",
      " test accuracy: 0.9178\n"
     ]
    }
   ],
   "source": [
    "import torch as T\n",
    "import torchvision as tv\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "\n",
    "# 定义数据生成器\n",
    "def load_data(batch_size):\n",
    "    # 将图像从 PIL 格式或 NumPy 数组转换为 PyTorch 的张量（tensor）\n",
    "    trans = [transforms.ToTensor()]\n",
    "    # 是一个用于将多个转换操作组合在一起的工具\n",
    "    trans = transforms.Compose(trans)\n",
    "    # 加载mnist数据集\n",
    "    train_data = tv.datasets.MNIST(root='./data',train=True,download=True,transform=trans)\n",
    "    test_data = tv.datasets.MNIST(root='./data',train=False,download=True,transform=trans)\n",
    "    # 定义数据加载器\n",
    "    train_iter = data.DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
    "    test_iter = data.DataLoader(test_data,batch_size=batch_size,shuffle=False)\n",
    "    return train_iter,test_iter\n",
    "\n",
    "# 定义网络\n",
    "net = nn.Sequential(nn.Flatten(),\n",
    "                    nn.Linear(28*28,10))\n",
    "# 初始化函数\n",
    "def initialize_weights(model):\n",
    "    for layer in model:\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            init.kaiming_uniform_(layer.weight, nonlinearity='relu')  # 使用 He 初始化\n",
    "            if layer.bias is not None:\n",
    "                init.zeros_(layer.bias)  # 偏置初始化为 0\n",
    "# 损失函数\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# 调用初始化函数\n",
    "initialize_weights(net)\n",
    "\n",
    "# 优化器\n",
    "sgd = T.optim.SGD(net.parameters(),lr=0.1)\n",
    "\n",
    "# 训练\n",
    "epoch = 10\n",
    "batch_size = 256\n",
    "train_iter,test_iter = load_data(batch_size)\n",
    "for i in range(epoch):\n",
    "    for X,y in train_iter:\n",
    "        sgd.zero_grad()\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat,y)\n",
    "        l.backward()\n",
    "        sgd.step()\n",
    "    with T.no_grad():\n",
    "        # 计算损失\n",
    "        tran_l = loss(net(X),y)\n",
    "        # 在测试集上计算分类正确率\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for X, y in test_iter:\n",
    "            y_hat = net(X)\n",
    "            predicted = T.argmax(y_hat, dim=1)  # 获取预测标签\n",
    "            correct += (predicted == y).sum().item()  # 统计正确的预测数量\n",
    "            total += y.size(0)  # 统计总样本数量\n",
    "        accuracy = correct / total\n",
    "        print(\"epoch:\",i + 1,\"\\n\",\"loss:\",tran_l.mean().numpy(),\"\\n\",\n",
    "              \"test accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472a0b3f",
   "metadata": {},
   "source": [
    "## 3.MLP的pytorch实现\n",
    "* 这里直接调用api实现，可以看到在添加了隐藏层后的accuracy大大提升了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f17c140e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 \n",
      " loss: 0.32476017 \n",
      " test accuracy: 0.9032\n",
      "epoch: 2 \n",
      " loss: 0.17184621 \n",
      " test accuracy: 0.9243\n",
      "epoch: 3 \n",
      " loss: 0.18020672 \n",
      " test accuracy: 0.9329\n",
      "epoch: 4 \n",
      " loss: 0.08596462 \n",
      " test accuracy: 0.939\n",
      "epoch: 5 \n",
      " loss: 0.17072274 \n",
      " test accuracy: 0.9445\n",
      "epoch: 6 \n",
      " loss: 0.18984102 \n",
      " test accuracy: 0.9454\n",
      "epoch: 7 \n",
      " loss: 0.08353443 \n",
      " test accuracy: 0.9511\n",
      "epoch: 8 \n",
      " loss: 0.09649221 \n",
      " test accuracy: 0.9521\n",
      "epoch: 9 \n",
      " loss: 0.20344277 \n",
      " test accuracy: 0.9562\n",
      "epoch: 10 \n",
      " loss: 0.21012758 \n",
      " test accuracy: 0.9605\n"
     ]
    }
   ],
   "source": [
    "import torch as T\n",
    "import torchvision as tv\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "\n",
    "# 定义数据生成器\n",
    "def load_data(batch_size):\n",
    "    # 将图像从 PIL 格式或 NumPy 数组转换为 PyTorch 的张量（tensor）\n",
    "    trans = [transforms.ToTensor()]\n",
    "    # 是一个用于将多个转换操作组合在一起的工具\n",
    "    trans = transforms.Compose(trans)\n",
    "    # 加载mnist数据集\n",
    "    train_data = tv.datasets.MNIST(root='./data',train=True,download=True,transform=trans)\n",
    "    test_data = tv.datasets.MNIST(root='./data',train=False,download=True,transform=trans)\n",
    "    # 定义数据加载器\n",
    "    train_iter = data.DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
    "    test_iter = data.DataLoader(test_data,batch_size=batch_size,shuffle=False)\n",
    "    return train_iter,test_iter\n",
    "\n",
    "# 定义网络\n",
    "net = nn.Sequential(nn.Flatten(),\n",
    "                    nn.Linear(28*28,256),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(256,10)\n",
    "                    )\n",
    "\n",
    "# 初始化函数\n",
    "def initialize_weights(model):\n",
    "    for layer in model:\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            init.kaiming_uniform_(layer.weight, nonlinearity='relu')  # 使用 He 初始化\n",
    "            if layer.bias is not None:\n",
    "                init.zeros_(layer.bias)  # 偏置初始化为 0\n",
    "# 损失函数\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# 调用初始化函数\n",
    "initialize_weights(net)\n",
    "\n",
    "# 优化器\n",
    "sgd = T.optim.SGD(net.parameters(),lr=0.1)\n",
    "\n",
    "# 训练\n",
    "epoch = 10\n",
    "batch_size = 256\n",
    "train_iter,test_iter = load_data(batch_size)\n",
    "for i in range(epoch):\n",
    "    for X,y in train_iter:\n",
    "        sgd.zero_grad()\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat,y)\n",
    "        l.backward()\n",
    "        sgd.step()\n",
    "    with T.no_grad():\n",
    "        # 计算损失\n",
    "        tran_l = loss(net(X),y)\n",
    "        # 在测试集上计算分类正确率\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for X, y in test_iter:\n",
    "            y_hat = net(X)\n",
    "            predicted = T.argmax(y_hat, dim=1)  # 获取预测标签\n",
    "            correct += (predicted == y).sum().item()  # 统计正确的预测数量\n",
    "            total += y.size(0)  # 统计总样本数量\n",
    "        accuracy = correct / total\n",
    "        print(\"epoch:\",i + 1,\"\\n\",\"loss:\",tran_l.mean().numpy(),\"\\n\",\n",
    "              \"test accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c614625e",
   "metadata": {},
   "source": [
    "* 暂退法（dropout）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed87540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])\n",
      "tensor([[ 0.,  2.,  4.,  0.,  0.,  0.,  0.,  0.],\n",
      "        [16., 18., 20.,  0., 24.,  0.,  0., 30.]])\n"
     ]
    }
   ],
   "source": [
    "import torch as T\n",
    "from torch import nn\n",
    "\n",
    "def dropout(X,drop):\n",
    "    assert 0 <= drop <= 1\n",
    "    if drop == 0:\n",
    "        return X\n",
    "    elif drop == 1:\n",
    "        return T.zeros_like(X)\n",
    "    else:\n",
    "        mask = T.rand_like(X) > drop # 随机丢弃\n",
    "        return mask * X / (1 - drop) # 保持期望值不变\n",
    "\n",
    "X = T.arange(16).reshape((2, 8)).float()\n",
    "print(X)\n",
    "print(dropout(X, 0.5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f794e9",
   "metadata": {},
   "source": [
    "* 自定义块以实现逻辑控制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7689f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# 继承自nn.Module，只需要实例化forward即可使用\n",
    "class MyBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(10, 10)\n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        X = F.relu(X)\n",
    "        if X.abs().sum() > 100:\n",
    "            return X * 0.5\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f50393e",
   "metadata": {},
   "source": [
    "* 参数管理和访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b965bf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[ 0.3237, -0.1566, -0.1963, -0.0686, -0.2525,  0.3526, -0.3255,  0.1239]])), ('bias', tensor([-0.0416]))])\n",
      "tensor([[ 0.3237, -0.1566, -0.1963, -0.0686, -0.2525,  0.3526, -0.3255,  0.1239]])\n",
      "tensor([-0.0416])\n"
     ]
    }
   ],
   "source": [
    "import torch as T\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4,8), nn.ReLU(),nn.Linear(8,1))\n",
    "X = T.rand(2, 4)\n",
    "\n",
    "print(net[2].state_dict())\n",
    "print(net[2].weight.data)\n",
    "print(net[2].bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32d6b0b",
   "metadata": {},
   "source": [
    "* 参数初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6347435c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0810,  0.6046, -0.3657,  0.6022,  0.0129, -0.1204,  0.1115, -0.6400]])\n",
      "tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "def init_Xavier(model):\n",
    "    for layer in model:\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            nn.init.zeros_(layer.bias)\n",
    "\n",
    "init_Xavier(net)\n",
    "print(net[2].weight.data)\n",
    "print(net[2].bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2bab5f",
   "metadata": {},
   "source": [
    "* GPU运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287df8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as T\n",
    "from torch import nn\n",
    "\n",
    "T.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be8154e",
   "metadata": {},
   "source": [
    "* 使用GPU训练MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e208d892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 \n",
      " loss: 0.3116208612918854 \n",
      " test accuracy: 0.9121\n",
      "epoch: 2 \n",
      " loss: 0.1257956475019455 \n",
      " test accuracy: 0.9236\n",
      "epoch: 3 \n",
      " loss: 0.1763961762189865 \n",
      " test accuracy: 0.9335\n",
      "epoch: 4 \n",
      " loss: 0.0909632071852684 \n",
      " test accuracy: 0.94\n",
      "epoch: 5 \n",
      " loss: 0.1892259269952774 \n",
      " test accuracy: 0.9418\n",
      "epoch: 6 \n",
      " loss: 0.17866913974285126 \n",
      " test accuracy: 0.9466\n",
      "epoch: 7 \n",
      " loss: 0.11648309230804443 \n",
      " test accuracy: 0.9485\n",
      "epoch: 8 \n",
      " loss: 0.0661228820681572 \n",
      " test accuracy: 0.9538\n",
      "epoch: 9 \n",
      " loss: 0.11011847853660583 \n",
      " test accuracy: 0.9556\n",
      "epoch: 10 \n",
      " loss: 0.09546028822660446 \n",
      " test accuracy: 0.9587\n"
     ]
    }
   ],
   "source": [
    "import torch as T\n",
    "import torchvision as tv\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "\n",
    "# 检查是否有可用的 GPU\n",
    "device = T.device('cuda' if T.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 定义数据生成器\n",
    "def load_data(batch_size):\n",
    "    # 将图像从 PIL 格式或 NumPy 数组转换为 PyTorch 的张量（tensor）\n",
    "    trans = [transforms.ToTensor()]\n",
    "    # 是一个用于将多个转换操作组合在一起的工具\n",
    "    trans = transforms.Compose(trans)\n",
    "    # 加载mnist数据集\n",
    "    train_data = tv.datasets.MNIST(root='./data', train=True, download=True, transform=trans)\n",
    "    test_data = tv.datasets.MNIST(root='./data', train=False, download=True, transform=trans)\n",
    "    # 定义数据加载器\n",
    "    train_iter = data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_iter = data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "    return train_iter, test_iter\n",
    "\n",
    "# 定义网络并移动到 GPU\n",
    "net = nn.Sequential(nn.Flatten(),\n",
    "                    nn.Linear(28 * 28, 256),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(256, 10)\n",
    "                    ).to(device)  # 将模型移动到 GPU\n",
    "\n",
    "# 初始化函数\n",
    "def initialize_weights(model):\n",
    "    for layer in model:\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            init.kaiming_uniform_(layer.weight, nonlinearity='relu')  # 使用 He 初始化\n",
    "            if layer.bias is not None:\n",
    "                init.zeros_(layer.bias)  # 偏置初始化为 0\n",
    "\n",
    "# 损失函数\n",
    "loss = nn.CrossEntropyLoss().to(device)  # 将损失函数移动到 GPU\n",
    "\n",
    "# 调用初始化函数\n",
    "initialize_weights(net)\n",
    "\n",
    "# 优化器\n",
    "sgd = T.optim.SGD(net.parameters(), lr=0.1)\n",
    "\n",
    "# 训练\n",
    "epoch = 10\n",
    "batch_size = 256\n",
    "train_iter, test_iter = load_data(batch_size)\n",
    "for i in range(epoch):\n",
    "    for X, y in train_iter:\n",
    "        X, y = X.to(device), y.to(device)  # 将数据移动到 GPU\n",
    "        sgd.zero_grad()\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "        l.backward()\n",
    "        sgd.step()\n",
    "    \n",
    "    with T.no_grad():\n",
    "        # 计算损失\n",
    "        tran_l = loss(net(X), y)\n",
    "        # 在测试集上计算分类正确率\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for X, y in test_iter:\n",
    "            X, y = X.to(device), y.to(device)  # 将数据移动到 GPU\n",
    "            y_hat = net(X)\n",
    "            predicted = T.argmax(y_hat, dim=1)  # 获取预测标签\n",
    "            correct += (predicted == y).sum().item()  # 统计正确的预测数量\n",
    "            total += y.size(0)  # 统计总样本数量\n",
    "        accuracy = correct / total\n",
    "        print(\"epoch:\", i + 1, \"\\n\", \"loss:\", tran_l.item(), \"\\n\", \"test accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0538265",
   "metadata": {},
   "source": [
    ">由上述结果可以看出运行结果确实快了不少\n",
    ">需要将网络，数据，标签都移动到GPU上\n",
    "```python\n",
    "# 定义设备\n",
    "device = T.device('cuda' if T.cuda.is_available() else 'cpu')\n",
    "# 移动网络到GPU\n",
    "net.to(device)\n",
    "# 移动数据到GPU\n",
    "X = X.to(device)\n",
    "y = y.to(device)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
